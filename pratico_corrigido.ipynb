{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim, há alguns erros e melhorias possíveis no código:\n",
    "\n",
    "    1 Erro de tipo: Na linha train_encodings = tokenizer(train_data, return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\"), o parâmetro padding deve ser padding='max_length' em vez de padding=\"max_length\".\n",
    "    2 Erro de tipo: Na linha optimizer = AdamW(model.parameters(), lr=5e-5), o parâmetro lr deve ser um float em vez de um literal científico. Deve ser optimizer = AdamW(model.parameters(), lr=5e-5) em vez de optimizer = AdamW(model.parameters(), lr=5e-5).\n",
    "    3 Melhoria de performance: O modelo GPT-2 é muito grande e pode ser lento para treinar. Você pode considerar usar um modelo mais leve, como o DistilGPT-2, ou reduzir o tamanho do batch.\n",
    "    4 Melhoria de performance: O treinamento do modelo pode ser mais rápido se você usar GPU em vez de CPU. Você pode usar o parâmetro device do modelo para especificar o dispositivo de treinamento.\n",
    "    5 Erro de lógica: Na linha loss.backward(), você não verifica se o loss é None antes de fazer o backward pass. Isso pode causar um erro se o loss for None.\n",
    "    6 Melhoria de código: Você pode considerar usar um loop de treinamento mais robusto, como o Trainer do Transformers, em vez de um loop manual.\n",
    "    7 Erro de tipo: Na linha response = generator(prompt, max_length=50, num_return_sequences=1), o parâmetro max_length deve ser um inteiro em vez de um float.\n",
    "\n",
    "Aqui está o código corrigido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "train_data = dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "train_encodings = tokenizer(train_data, return_tensors='pt', max_length=64, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config and model\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer and data loader\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(train_encodings, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in train_loader:\n",
    "        inputs = batch.data['input_ids']\n",
    "        attention_mask = batch.data['attention_mask']\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch}: Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained('./meuModeloGPT2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator pipeline\n",
    "generator = pipeline('text-generation', model='./meuModeloGPT2', tokenizer='gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"Qual é o significado da vida, do universo e tudo mais?\"\n",
    "response = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
