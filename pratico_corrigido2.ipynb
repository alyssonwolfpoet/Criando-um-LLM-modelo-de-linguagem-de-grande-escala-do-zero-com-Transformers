{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim, há mais melhorias possíveis no código:\n",
    "\n",
    "   1 Utilizar Trainer do Transformers: Em vez de implementar um loop de treinamento manual, você pode utilizar o Trainer do Transformers, que é mais robusto e fácil de usar.\n",
    "   2 Utilizar DataCollator do Transformers: Em vez de criar um DataLoader manualmente, você pode utilizar o DataCollator do Transformers, que é mais fácil de usar e mais eficiente.\n",
    "   3 Utilizar Accelerator do Transformers: Em vez de treinar o modelo em um dispositivo específico, você pode utilizar o Accelerator do Transformers, que é mais fácil de usar e mais eficiente.\n",
    "   4 Adicionar mais hiperparâmetros: Você pode adicionar mais hiperparâmetros para ajustar o modelo, como o tamanho do batch, o número de epochs, o learning rate, etc.\n",
    "   5 Utilizar EarlyStopping: Você pode utilizar EarlyStopping para parar o treinamento quando o modelo atinge um certo nível de performance.\n",
    "   6 Utilizar ModelCheckpoint: Você pode utilizar ModelCheckpoint para salvar o modelo em intervalos regulares durante o treinamento.\n",
    "   7 Adicionar mais métricas de avaliação: Você pode adicionar mais métricas de avaliação, como a perda de validação, a precisão, a revocação, etc.\n",
    "   8 Utilizar TensorBoard: Você pode utilizar TensorBoard para visualizar o treinamento do modelo e avaliar seu desempenho.\n",
    "   9 Adicionar mais comentários e documentação: Você pode adicionar mais comentários e documentação para explicar o código e torná-lo mais fácil de entender.\n",
    "   10 Utilizar Type Hints: Você pode utilizar Type Hints para especificar os tipos de variáveis e parâmetros, tornando o código mais legível e fácil de entender.\n",
    "\n",
    "Aqui está o código com algumas dessas melhorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "train_data = dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data\n",
    "train_encodings = tokenizer(train_data, return_tensors='pt', max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config and model\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./meuModeloGPT2',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,\n",
    "    save_strategy='steps',\n",
    "    eval_accumulation_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encodings,\n",
    "    eval_dataset=train_encodings,\n",
    "    compute_metrics=lambda pred: {'loss': pred.loss},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator pipeline\n",
    "generator = pipeline('text-generation', model='./meuModeloGPT2', tokenizer='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"Qual é o significado da vida, do universo e tudo mais?\"\n",
    "response = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
