{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melhorias adicionadas:\n",
    "\n",
    "    Adicionado comentários em pt-br para explicar o código\n",
    "    Utilizado TrainingArguments para definir argumentos de treinamento\n",
    "    Utilizado Trainer para treinar o modelo\n",
    "    Adicionado compute_metrics para calcular métricas durante o treinamento\n",
    "    Utilizado pipeline para criar uma pipeline de geração de texto\n",
    "    Adicionado num_return_sequences para especificar o número de sequências a serem geradas\n",
    "\n",
    "Essas são apenas algumas sugestões de melhorias, e há muitas outras coisas que você pode fazer para melhorar o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alysson/workspace/Criando-um-LLM-modelo-de-linguagem-de-grande-escala-do-zero-com-Transformers/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "train_data = dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar dados\n",
    "train_encodings = tokenizer(\n",
    "    train_data,\n",
    "    return_tensors='pt',\n",
    "    max_length=1,\n",
    "    truncation=True,\n",
    "    padding='max_length'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar configuração do modelo\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar modelo\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Criar argumentos de treinamento\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./meuModeloGPT2\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[1;32m      4\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,  \n\u001b[1;32m      6\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,  \n\u001b[1;32m      7\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[1;32m      8\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,  \n\u001b[1;32m      9\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \n\u001b[1;32m     10\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Changed from 'teps' to 'epoch'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[1;32m     12\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[1;32m     13\u001b[0m     greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \n\u001b[1;32m     14\u001b[0m     eval_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "# Criar argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./meuModeloGPT2',  \n",
    "    num_train_epochs=5,  \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,  \n",
    "    evaluation_strategy='epoch',  \n",
    "    learning_rate=5e-5,  \n",
    "    save_total_limit=2,  \n",
    "    save_strategy='epoch',  # Changed from 'teps' to 'epoch'\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model='loss',  \n",
    "    greater_is_better=False,  \n",
    "    eval_accumulation_steps=10,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar treinador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encodings,\n",
    "    eval_dataset=train_encodings,\n",
    "    compute_metrics=lambda pred: {'loss': pred.loss}  # Função para calcular métricas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pipeline de geração de texto\n",
    "generator = pipeline('text-generation', model='./meuModeloGPT2', tokenizer='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar texto\n",
    "prompt = \"Qual é o significado da vida, do universo e tudo mais?\"\n",
    "response = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(response[0]['generated_text'])  # Imprimir texto gerado\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
